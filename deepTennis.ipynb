{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Reach</h1>\n",
    "\n",
    "The following implementation is inspired by <h><a href=\"https://github.com/SimonBirrell/ppo-continuous-control\">ppo-continuous-control</a></h> and <h><a href=\"https://github.com/higgsfield/RL-Adventure-2\">RL-Adventure-2</a></h>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import, configure, and load reacher environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Setup Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# Setup Cuda or CPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Start Environment\n",
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"./unity_simulation/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Examine the reacher environement</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 275           # Range 32 to 5000\n",
    "DISCOUNT_FACTOR = 0.99  # Range 0.8 to 0.9997\n",
    "GAE_DISCOUNT = 0.99     # Range 0.8 to 0.9997\n",
    "GAE_LAMBDA = 0.95       # Range 0.9 to 1\n",
    "EPOCH_RANGE = 12        # Range 3 to 30\n",
    "MINIBATCH_SIZE = 64     # Range 4 to 4096\n",
    "PPO_CLIP_RANGE = 0.15   # Range 0.05 to 0.3 [testing --> 0.05; 0.1; 0.15]\n",
    "LEARNING_RATE = 3e-4    # Range 0.003 to 5e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Utilities</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is inspired/copied from:\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network (actor/critic)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_agents, state_size, action_size, hidden_size, seed):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size))\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = torch.tensor(obs, device=DEVICE, dtype=torch.float32)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        dist = torch.distributions.Normal(a, self.log_std.exp().expand_as(a))\n",
    "        return (v, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rollout manager</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout_manager():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored data\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        \n",
    "        # Calculated data\n",
    "        self.returns = [0.0] * HORIZON\n",
    "        self.advantages = [0.0] * HORIZON\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(HORIZON)):\n",
    "            returns = self.rewards[i] + DISCOUNT_FACTOR * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = torch.tensor(np.zeros((num_agents, 1)), device=DEVICE, dtype=torch.float32)\n",
    "        for i in reversed(range(HORIZON)):\n",
    "            td = self.rewards[i] + (GAE_DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * GAE_DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:HORIZON], dim=0)\n",
    "            \n",
    "    def get_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PPO algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_algorithm():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, hidden_size, seed):\n",
    "        self.network = ActorCritic(num_agents, state_size, action_size, hidden_size, seed).to(DEVICE)\n",
    "        self.first_states = True\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = action_distribution.log_prob(self.latest_actions).sum(-1).unsqueeze(-1)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float32).unsqueeze(-1)\n",
    "        episode_not_dones = torch.tensor((1 - dones), device=DEVICE, dtype=torch.float32).unsqueeze(-1)\n",
    "        states = torch.tensor(self.states, device=DEVICE, dtype=torch.float32)      \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "        self.states = next_states\n",
    "                \n",
    "    def create_rollout(self):\n",
    "        self.rollout = Rollout_manager()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        # save final results\n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        # Calculate resutns and advantages\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        # Optimize\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "   \n",
    "    def save_weights(self):\n",
    "        torch.save(self.network.state_dict(), \"trained_weights.pth\")\n",
    "        print(\"Weights are saved to trained_weights.pth\")\n",
    "        \n",
    "    def ppo_update(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        # Policy Loss\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = ratio.clamp(1.0 - PPO_CLIP_RANGE, 1.0 + PPO_CLIP_RANGE) * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        \n",
    "        # Value Loss\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        \n",
    "        # Optimize network weights\n",
    "        loss = policy_loss + value_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), 0.75) \n",
    "        optimizer.step()\n",
    "\n",
    "    def optimize(self):\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), LEARNING_RATE, eps=1e-5)\n",
    "        for i in range(EPOCH_RANGE):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINIBATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = torch.tensor(timestep, device=DEVICE, dtype=torch.float32).long()\n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.ppo_update(optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train agent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_agent():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.running_mean_score = 0\n",
    "        self.mean_scores = []\n",
    "                \n",
    "    def train_ppo(self, agent, target_avg_score, max_episodes=300, train_mode_=True):\n",
    "        if train_mode_:\n",
    "            print(\"========================================================================================\")\n",
    "            print(\"Beginning training...\")\n",
    "            print(\"GOAL: Mean episode score of {:.2f} over the last 100 episodes in less than {} episodes.\".format(target_avg_score, max_episodes))\n",
    "            print(\"========================================================================================\")\n",
    "        else:\n",
    "            print(\"Testing Agent\")\n",
    "        \n",
    "        # Reset environment:\n",
    "        env_info = env.reset(train_mode=train_mode_)[brain_name]\n",
    "        \n",
    "        # Initialize variables:\n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_avg_score = target_avg_score\n",
    "        \n",
    "        while True:\n",
    "            # Run a rollout (max HORIZON)\n",
    "            states = env_info.vector_observations # Get current states\n",
    "            scores = np.zeros(num_agents) # Initialize scores list\n",
    "            agent.create_rollout() # Create rollout instance\n",
    "            \n",
    "            horizon_ = HORIZON\n",
    "\n",
    "            for t in range(horizon_):\n",
    "                # Get actions from policy\n",
    "                actions = np.clip(agent.get_prediction(states).cpu().detach().numpy(), -1, 1) # Limit max torque between -1 and 1\n",
    "\n",
    "                # Use the actions in the environemnt and get next state\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "\n",
    "                # Save rewards\n",
    "                self.online_rewards += rewards\n",
    "                for i, done in enumerate(dones):\n",
    "                    # If a worker is done end the episode for this worker\n",
    "                    if done:\n",
    "                        # Save the workers reward from the episode\n",
    "                        self.episode_scores[i].append(self.online_rewards[i])\n",
    "                        self.episodes_finished +=1\n",
    "                        # If all workers finished end the entire episode\n",
    "                        if (self.episodes_finished % num_agents) == 0:\n",
    "                            max_score = -99.0\n",
    "                            self.num_episodes += 1\n",
    "                            # Add up the rewards that each agent received and pick max\n",
    "                            for j in range(num_agents):\n",
    "                                single_agent_acore = self.episode_scores[i][-1]\n",
    "                                max_score = single_agent_acore if single_agent_acore > max_score else max_score\n",
    "                            self.last_100_scores.append(max_score)\n",
    "                            self.mean_scores.append(max_score)\n",
    "                            if train_mode_:\n",
    "                                print(\"Episode: %3.0f | Mean max score: %7.4f | Mean 100 episode score: %7.4f\" % (self.num_episodes, round(max_score, 6), round(np.mean(self.last_100_scores),6)))\n",
    "                        \n",
    "                        env_info = env.reset(train_mode=train_mode_)[brain_name]\n",
    "                        # Set accumulated reward to zero for next episode\n",
    "                        self.online_rewards[i] = 0\n",
    "                        # Calculate running mean\n",
    "                        self.running_mean_score = np.mean(self.last_100_scores)\n",
    "                        # Break if the target if reached \n",
    "                        if self.running_mean_score > self.target_avg_score and train_mode_:\n",
    "                            print(\" \")\n",
    "                            print(\"*** Target reached! ***\")\n",
    "                            print(\" \")\n",
    "                            break\n",
    "\n",
    "                agent.step(states, actions, rewards, next_states, dones) \n",
    "                scores += rewards\n",
    "                states = next_states\n",
    "                \n",
    "            # Optimize the agent using PPO and the data from the last rollout\n",
    "            agent.process_rollout(states)\n",
    "\n",
    "            # Check exit conditions\n",
    "            if self.running_mean_score > target_avg_score and train_mode_:\n",
    "                print(\"Mean episode max score of %s over the last 100 episodes\" % self.running_mean_score)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes and train_mode_:\n",
    "                print(\"Failed mean episode max score of {:.2f} over the 100 most recent episodes in less than {} episodes.\".format(target_avg_score, self.num_episodes))\n",
    "                agent.save_weights() \n",
    "                break\n",
    "        return self.mean_scores           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train PPO agent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore Rimtime Warnings\n",
    "\n",
    "scores        = []\n",
    "for i in range(5):\n",
    "    agent         = PPO_algorithm(num_agents, state_size=state_size, action_size=action_size, hidden_size=32, seed=123)\n",
    "    train_session = train_agent(num_agents)\n",
    "    scores.append(train_session.train_ppo(agent, 100, 2500))\n",
    "    \n",
    "# Save scores to csv file:\n",
    "episodes = []\n",
    "scores_con = []\n",
    "for i in range(len(scores)):\n",
    "    episodes += range(len(scores[i]))\n",
    "    scores_con += scores[i]\n",
    "    \n",
    "savetxt('./score.csv', np.c_[episodes,scores_con], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Setup seaborn for plotting\n",
    "sns.set(\"talk\", font_scale=1.5,  rc={\"lines.linewidth\": 3, 'figure.figsize': (14, 7)})\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Get and arrange data\n",
    "df = pd.read_csv(\"./score.csv\")      # Read CSV file generated while training\n",
    "df.columns = [\"episodes\", \"scores\"]  # Rename columns \n",
    "\n",
    "# Plot data\n",
    "sns_plot = sns.lineplot('episodes', 'scores', data=df)\n",
    "sns_plot.set(ylabel='Score', xlabel='Episode')\n",
    "figure = sns_plot.get_figure()\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot scores </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Setup seaborn for plotting\n",
    "sns.set(\"talk\", font_scale=1.5,  rc={\"lines.linewidth\": 3, 'figure.figsize': (14, 7)})\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_style(\"whitegrid\")\n",
    " \n",
    "# Get and arrange data\n",
    "df05 = pd.read_csv(\"./data/score_0_05.csv\")      # Read CSV file generated while training\n",
    "df05.columns = [\"episodes\", \"scores\"]  # Rename columns \n",
    "df_scores_smooth05 = df05.groupby([\"episodes\"], as_index=False).mean()        # Prepare for rolling mean\n",
    "df_scores_smooth05['smooth_score'] = df_scores_smooth05['scores'].rolling(100).mean() # Use rolling mean of 15\n",
    "\n",
    "df1 = pd.read_csv(\"./data/score_0_1.csv\")      # Read CSV file generated while training\n",
    "df1.columns = [\"episodes\", \"scores\"]  # Rename columns\n",
    "df_scores_smooth1 = df1.groupby([\"episodes\"], as_index=False).mean()        # Prepare for rolling mean\n",
    "df_scores_smooth1['smooth_score'] = df_scores_smooth1['scores'].rolling(100).mean() # Use rolling mean of 15\n",
    "\n",
    "df15 = pd.read_csv(\"./data/score_0_15.csv\")      # Read CSV file generated while training\n",
    "df15.columns = [\"episodes\", \"scores\"]  # Rename columns \n",
    "df_scores_smooth15 = df15.groupby([\"episodes\"], as_index=False).mean()        # Prepare for rolling mean\n",
    "df_scores_smooth15['smooth_score'] = df_scores_smooth15['scores'].rolling(100).mean() # Use rolling mean of 15\n",
    "\n",
    "\n",
    "# Plot data\n",
    "sns_plot = sns.lineplot('episodes', 'scores', data=df05, linewidth = 1, color=\"green\", alpha = 0.5, label='_nolegend_')\n",
    "sns_plot = sns.lineplot('episodes', 'scores', data=df1, linewidth = 1, color=\"orange\", alpha = 0.5, label='_nolegend_')\n",
    "sns_plot = sns.lineplot('episodes', 'scores', data=df15, linewidth = 1, color=\"blue\", alpha = 0.5, label='_nolegend_')\n",
    "\n",
    "sns_plot = sns.lineplot('episodes', 'smooth_score', data=df_scores_smooth05, linewidth = 3.5, color=\"green\")\n",
    "sns_plot = sns.lineplot('episodes', 'smooth_score', data=df_scores_smooth1, linewidth = 3.5, color=\"orange\")\n",
    "sns_plot = sns.lineplot('episodes', 'smooth_score', data=df_scores_smooth15, linewidth = 3.5, color=\"blue\")\n",
    "\n",
    "sns_plot.set(ylabel='Score', xlabel='Episode')\n",
    "sns_plot.legend(fontsize = 'small', loc='top left', labels=[\"PPO_CLIP_RANGE=0.05\", \"PPO_CLIP_RANGE=0.10\", \"PPO_CLIP_RANGE=0.15\"], framealpha = 0)\n",
    "figure = sns_plot.get_figure()\n",
    "figure.tight_layout()\n",
    "figure.savefig('./score_episode.png', dpi=200) # Save figure to score.png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load and test trained agent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mat/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mat/.local/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-80aacea06483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_models/trained_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m99.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1b8f254aefef>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(self, agent, target_avg_score, max_episodes, train_mode_)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# Use the actions in the environemnt and get next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mrl_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_brain_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0magents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0mlocal_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mvector_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored_vector_actions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m                 \u001b[0mtext_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored_text_actions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0mmax_reached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_step_reached\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent         = PPO_algorithm(num_agents, state_size=state_size, action_size=action_size, hidden_size=32, seed=123)\n",
    "agent.network.load_state_dict(torch.load('./saved_models/trained_weights.pth'))\n",
    "train_session = train_agent(num_agents)\n",
    "train_session.train_ppo(agent, 99.0, 300, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
